{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ef9e2d3-d542-447d-b153-dea8de3b9fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\apps\\\\opt\\\\spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ac918d3-b3a0-45cc-bd70-4e8f9678a015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>transaction_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>GENERIC_VAT_ON_PSITEK_COMMISSION</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>GENERIC_SD_COMMISSION</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>GENERIC_VAT_ON_SD_COMMISSION</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>GENERIC_DEVICE_COMMISSION</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>GENERIC_VAT_ON_DEVICE_COMMISSION</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     transaction_type transaction_id\n",
       "825  GENERIC_VAT_ON_PSITEK_COMMISSION            202\n",
       "826             GENERIC_SD_COMMISSION            203\n",
       "827      GENERIC_VAT_ON_SD_COMMISSION            204\n",
       "828         GENERIC_DEVICE_COMMISSION            205\n",
       "829  GENERIC_VAT_ON_DEVICE_COMMISSION            206"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "429c3ca0-6179-4ccc-b209-bc16b49648ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|    transaction_type|transaction_primary_type_id|\n",
      "+--------------------+---------------------------+\n",
      "|             DEPOSIT|                          0|\n",
      "|          WITHDRAWAL|                          1|\n",
      "|        RESET_PROFIT|                          2|\n",
      "|       LOCAL_CONTENT|                          3|\n",
      "|    RELATED_TRANSFER|                          4|\n",
      "|      STOCK_PURCHASE|                          5|\n",
      "|          STOCK_SALE|                          6|\n",
      "|   DEVICE_COMMISSION|                          7|\n",
      "| CUSTOMER_COMMISSION|                          8|\n",
      "|            REVERSAL|                          9|\n",
      "|        VAT_ON_SALES|                         10|\n",
      "|             JOURNAL|                         11|\n",
      "|             PAYMENT|                         12|\n",
      "|                 CDF|                         13|\n",
      "|VAT_ON_CUSTOMER_C...|                         14|\n",
      "|          VAT_ON_CDF|                         15|\n",
      "|        VAT_ON_STOCK|                         16|\n",
      "|          CDF_REFUND|                         17|\n",
      "|   SALE_PSITEK_TO_SD|                         18|\n",
      "|   SALE_SD_TO_VENDOR|                         19|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Wednesday Feb 1 2023\n",
    "@author: Olabode Ajayi\n",
    "\"\"\"\n",
    "#import required modules and pandas library\n",
    "from pyspark.sql import SparkSession\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def normalize_space(my_s):\n",
    "    \"\"\"Return s stripped of leading/trailing whitespace\n",
    "    and with internal runs of whitespace replaced by a single SPACE\"\"\"\n",
    "    # remove all the occurrences of the TRANSACTION_TYPE_ character:\n",
    "    # This should be a str method :-(\n",
    "    my_new_string = my_s.replace(\"TRANSACTION_TYPE_\", \"\")\n",
    "    return ' '.join(my_new_string.split())\n",
    "\n",
    "\n",
    "with open('kazang_transaction_types.py','r') as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "# Create Dictonary and read line by line content of the file\n",
    "my_dict = {}\n",
    "\n",
    "for line in content:\n",
    "    split = line.split('=')\n",
    "    result= [normalize_space(i) for i in split]\n",
    "    #print(result)\n",
    "    my_dict[result[0]] = result[1]\n",
    "\n",
    "    \n",
    "# creating a Dataframe object from a list \n",
    "# of tuples of key, value pair\n",
    "df = pd.DataFrame(list(my_dict.items()))\n",
    "# Rename the column by assigning .columns attribute:\n",
    "df.columns = ['transaction_type', 'transaction_primary_type_id']\n",
    "df.to_csv('transaction_type.csv',sep=';',index=False) # Use semi-colon to seperate data and export to CSV format\n",
    "\n",
    "\n",
    "# initialize SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python ETL script for TEST\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\", '8g')\\\n",
    "    .config(\"spark.sql.ansi.enabled \",True)\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:\\apps\\opt\\spark-3.3.1-bin-hadoop3\\jars\\sqljdbc42.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Create spark configuration object\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"My app\")\n",
    " \n",
    "#Create spark context and sparksession\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    " \n",
    "#read csv file in a dataframe\n",
    "df = spark.read.csv(path=\"transaction_type.csv\", header = True, sep = \";\")\n",
    "\n",
    " \n",
    "#set variable to be used to connect the database\n",
    "\n",
    "table = \"[ArchiveRawData].[dbo].[transaction_type]\"\n",
    "database = \"ArchiveRawData\"\n",
    "user = \"kaz\"\n",
    "password  = \"Password12345\"\n",
    " \n",
    "#write the dataframe into a sql table\n",
    "df.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:sqlserver://localhost:1433;databaseName={database};encrypt=true;trustServerCertificate=true;\") \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "    .save()\n",
    "\n",
    "\n",
    "query_string= \"\"\"\n",
    "            SELECT *\n",
    "            FROM [ArchiveRawData].[dbo].[transaction_type]\n",
    "            \"\"\"\n",
    "\n",
    "#Read ETL Meta Data\n",
    "etl_meta_data_staging = spark.read\\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\\\n",
    "    .option(\"url\", \"jdbc:sqlserver://127.0.0.1:1433;databaseName={\"+database+\"};encrypt=true;trustServerCertificate=true;\") \\\n",
    "    .option(\"query\", query_string) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09034b-687b-4bad-95a4-52eae00aca35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
